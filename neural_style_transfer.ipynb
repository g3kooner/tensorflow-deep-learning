{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jME_9isSVAvw"
      },
      "source": [
        "# Neural Style Transfer\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In this Colab, we will be implementing neural style transfer to recreate a given image using the style properties of another. By utilizing the [VGG19](https://arxiv.org/abs/1409.1556) model to extract features from specific layers, we can calculate style and content losses to further improve our new generating image.\n",
        "\n",
        "![style](images/style.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkXA9Ct9U28C"
      },
      "source": [
        "# Imports and Data Preprocessing\n",
        "\n",
        "Importing the necessary libraries before continuing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vc26m4d8VA2R"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display as display_fn\n",
        "from keras import backend as K\n",
        "from IPython.display import Image, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnqDYWC9pmto"
      },
      "source": [
        " We define utility functions used in loading, displaying and preprocessing our style + content images before they can be inputted into our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hjy0O1AdVJnE"
      },
      "outputs": [],
      "source": [
        "def load_img(img_path, max_dim = 512):\n",
        "  image = tf.io.read_file(img_path)\n",
        "  image = tf.io.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  img_shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(img_shape)\n",
        "  scale_factor = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(img_shape * scale_factor, dtype=tf.int32)\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "  return image\n",
        "\n",
        "def display_imgs(images, titles=[]):\n",
        "  plt.figure(figsize=(18,10))\n",
        "  for idx, (image, title) in enumerate(zip(images,titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.axis('off')\n",
        "    if(len(tf.shape(image)) > 3): image = tf.squeeze(image)\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "\n",
        "def preprocess_img(image):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "  return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiuzpPAvcK2K"
      },
      "source": [
        "# Downloading Images\n",
        "\n",
        "The cells below will download a puppy dog image to use as our content and Van Goghs famous Starry Night painting to style with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2S7c1OvsyEW"
      },
      "outputs": [],
      "source": [
        "img_dir = 'images'\n",
        "if not os.path.exists(img_dir): os.makedirs(img_dir)\n",
        "\n",
        "!wget -q -O ./images/dog.jpg https://cdn.pixabay.com/photo/2020/06/30/22/34/dog-5357794__340.jpg\n",
        "!wget -q -O ./images/night.jpg https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1024px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\n",
        "\n",
        "content_path = f'{img_dir}/dog.jpg'\n",
        "style_path = f'{img_dir}/night.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E5QGWYks5wV"
      },
      "outputs": [],
      "source": [
        "content_img = load_img(content_path); style_img = load_img(style_path)\n",
        "display_imgs([content_img, style_img], [\"content img\", \"style img\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XcMopV5gb3t"
      },
      "source": [
        "# Creating the Model\n",
        "\n",
        "Below is an image displaying the architecture of the VGG-19 model we are using for feature extraction. We will need to select certain layers to output from that will be used to compute the losses for the image. Thus we will drop the fully-connected layers at the end for our purposes.\n",
        "\n",
        "![vgg](images/vgg.png)\n",
        "\n",
        "\n",
        "Due to the nature of convolutional networks, it makes sense to choose a layer deeper in the network for the content as they are able to learn more complex high level features. The style layers have been chosen as the first layer in each convolutional block through experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2zOQ6HSywcH"
      },
      "outputs": [],
      "source": [
        "content_layers = ['block5_conv2']\n",
        "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1','block5_conv1']\n",
        "\n",
        "combined_layers = style_layers + content_layers\n",
        "num_style_layers = len(style_layers); num_content_layers = len(content_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQQkWvlipsps"
      },
      "source": [
        "Now we can define and instantiate our model using the weight-frozen VGG-19 model, but only output from the chosen content and style layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSzp29dD2F8z"
      },
      "outputs": [],
      "source": [
        "def vgg_model(layers):\n",
        "  vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
        "  vgg19.trainable = False\n",
        "\n",
        "  output_layers = [vgg19.get_layer(layer).output for layer in layers]\n",
        "  model = tf.keras.Model(inputs=vgg19.input, outputs=output_layers)\n",
        "\n",
        "  return model\n",
        "\n",
        "vgg = vgg_model(combined_layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n32rGRXqUaT"
      },
      "source": [
        "# Loss Functions and Features\n",
        "\n",
        "Next we can define the loss functions associated with improving the generation of our mixed image. Here we can see the style loss is the reduce mean of the square of the element-wise subtraction between the features and targets, the content loss is the reduce sum instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkvCduk-3-kR"
      },
      "outputs": [],
      "source": [
        "def get_style_loss(features, targets):\n",
        "  style_loss = tf.reduce_mean(tf.square(features - targets))\n",
        "\n",
        "  return style_loss\n",
        "\n",
        "def get_content_loss(feature, target):\n",
        "  content_loss = tf.reduce_sum(tf.square(feature - target))\n",
        "\n",
        "  return content_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2SGBuyzxYbk"
      },
      "source": [
        "The paper on neural style transfer suggests representing the style feature maps as gram matrices which can be done using `tf.linalg.einsum`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPRadFji5Ymk"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(input):\n",
        "  matrix = tf.linalg.einsum('bijc,bijd->bcd', input, input)\n",
        "  average_factor = tf.cast(tf.shape(input)[1] * tf.shape(input)[2], tf.float32)\n",
        "  gram_matrix = matrix / average_factor\n",
        "\n",
        "  return gram_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phWfB7ZrzT4z"
      },
      "source": [
        "The two functions below will return the content and gram matrix style  feature maps for an image that will be passed to our loss functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TO2ZwKKm31aj"
      },
      "outputs": [],
      "source": [
        "def style_img_features(image):\n",
        "  preprocessed_image = preprocess_img(image)\n",
        "  model_output = vgg(preprocessed_image)\n",
        "  style_features = model_output[0:num_style_layers]\n",
        "  gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "\n",
        "  return gram_style_features\n",
        "\n",
        "def content_img_features(image):\n",
        "  preprocessed_image = preprocess_img(image)\n",
        "  model_output = vgg(preprocessed_image)\n",
        "  content_features = model_output[num_style_layers:]\n",
        "\n",
        "  return content_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8somS7_1KRn"
      },
      "source": [
        "The total loss is a weighted summation of the style and content loss together. We scale their individual losses based on the # of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjYo1KSN6Pm2"
      },
      "outputs": [],
      "source": [
        "def calculate_total_loss(style_features, content_features, style_targets, content_targets, style_weight, content_weight):\n",
        "  style_loss = tf.math.add_n([get_style_loss(feature, target) for (feature, target) in zip(style_features, style_targets)])\n",
        "  content_loss = tf.math.add_n([get_content_loss(feature, target) for (feature, target) in zip(content_features, content_targets)])\n",
        "\n",
        "  style_loss = style_loss * style_weight / num_style_layers\n",
        "  content_loss = content_loss * content_weight / num_content_layers\n",
        "  total_loss = style_loss + content_loss\n",
        "\n",
        "  return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72QbzJeyqsO_"
      },
      "source": [
        "# Compile and Fitting\n",
        "\n",
        "We are now ready to put the finishing touches to our model. The function below defines a singular training step in which we provide it our style and content targets/weights, calculate the gradients w.r.t the generated image using the loss and make updates to reduce it using an optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2wapsw3YfLY"
      },
      "outputs": [],
      "source": [
        "def train_step(image, style_targets, content_targets, style_weight, content_weight, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    style_features = style_img_features(image)\n",
        "    content_features = content_img_features(image)\n",
        "\n",
        "    loss = calculate_total_loss(style_features, content_features, style_targets, content_targets, style_weight, content_weight)\n",
        "\n",
        "  gradients = tape.gradient(loss, image)\n",
        "  optimizer.apply_gradients([(gradients, image)])\n",
        "  image.assign(tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=255.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS5W6cAF5J7X"
      },
      "source": [
        "Now we can loop the previously defined training step over a number of epochs and steps to generate, update and construct the final image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7GYew8rcg3T"
      },
      "outputs": [],
      "source": [
        "def model_fit(style_image, content_image, optimizer, epochs, steps_per_epoch, style_weight=1e-2, content_weight=1e-4):\n",
        "  style_targets = style_img_features(style_image)\n",
        "  content_targets = content_img_features(content_image)\n",
        "  generated_image = tf.Variable(tf.cast(content_image, dtype=tf.float32))\n",
        "\n",
        "  for i in range(epochs):\n",
        "    for j in range(steps_per_epoch):\n",
        "      train_step(generated_image, style_targets, content_targets, style_weight, content_weight, optimizer)\n",
        "\n",
        "    print(f\"Epoch {i}/{epochs}\")\n",
        "\n",
        "  generated_image = tf.cast(generated_image, dtype=tf.uint8)\n",
        "\n",
        "  return generated_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghr0fzJw5w2q"
      },
      "source": [
        "All thats left to do now is run the model_fit loop which should take ~8 minutes to complete with hardware acceleration and display the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FfqZ_A8hrAP"
      },
      "outputs": [],
      "source": [
        "style_weight = 3e-2; content_weight = 1e-2\n",
        "opt = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=20.0, decay_steps=100, decay_rate=0.5\n",
        "    )\n",
        ")\n",
        "\n",
        "neural_style_transfer = model_fit(style_img, content_img, opt, 10, 100, style_weight, content_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCu9WUrXiKPl"
      },
      "outputs": [],
      "source": [
        "display_imgs([content_img, style_img, neural_style_transfer], [\"content image\", \"style image\", \"final image\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

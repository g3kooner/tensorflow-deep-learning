{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0HYUsC72q9j"
      },
      "source": [
        "# Semantic Image Segmentation with UNet\n",
        "---\n",
        "\n",
        "\n",
        "In this Colab, we will build a famous model (UNet) that helps to predict segmentation masks (pixel-wise label maps) of various pets. We will train the model on the [Oxford-IIIT pet](https://www.robots.ox.ac.uk/~vgg/data/pets/) dataset that contains over 37 unique categories. [UNet](https://arxiv.org/abs/1505.04597) is a fully convolutional network which uses skip connections to join parallel encoder stages to the decoder. We will evaluate the models performance using IOU and dice score metrics.\n",
        "\n",
        "![segmentation](images/segmentation.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elTEGcd2lQce"
      },
      "source": [
        "# Imports and Data Preprocessing\n",
        "\n",
        "First, we must import the necessary modules to perform the given task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNthoxuUlSGM"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joBdtRv7-Pme"
      },
      "source": [
        "Next we will need to down the Oxford IIIT dataset that we will be training/testing upon. Luckily for us, the dataset is already included within the TensorFlow Datasets and we can retrieve it by running the cell below. This will include the pet images, bounding boxes and segmentation masks, but we will be ignoring the boxes for now. Note that the masks are only included in data versions 3+ which is why the name is \"3.\\*.\\*\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b07qMOqUlULn"
      },
      "outputs": [],
      "source": [
        "# download the dataset and get info\n",
        "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
        "\n",
        "# class list for the pixel maps\n",
        "class_names = ['pet', 'background', 'outline']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N65mu6R_y6Z"
      },
      "source": [
        "We define some utilities functions to help us with proprocessing the data including normalizing the pixel values to range between [0,1] for quicker convergence, data augmentation to improve our models generality and resizing images to the approriate size to feed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTDsOyGJmX2f"
      },
      "outputs": [],
      "source": [
        "def normalize(image, seg_mask):\n",
        "  image = tf.cast(image, dtype='float32') / 255.0\n",
        "  seg_mask -= 1\n",
        "  return image, seg_mask\n",
        "\n",
        "def simple_augmentation(image, seg_mask):\n",
        "  if random.randint(0, 9) > 5:\n",
        "    image = tf.image.flip_left_right(image)\n",
        "    seg_mask = tf.image.flip_left_right(seg_mask)\n",
        "\n",
        "  return image, seg_mask\n",
        "\n",
        "def process_training(data_entry):\n",
        "  image = tf.image.resize(data_entry['image'], (128, 128), 'nearest')\n",
        "  seg_mask = tf.image.resize(data_entry['segmentation_mask'], (128, 128), 'nearest')\n",
        "  image, seg_mask = simple_augmentation(image, seg_mask)\n",
        "  image, seg_mask = normalize(image, seg_mask)\n",
        "\n",
        "  return image, seg_mask\n",
        "\n",
        "def process_test(data_entry):\n",
        "  image = tf.image.resize(data_entry['image'], (128, 128), 'nearest')\n",
        "  seg_mask = tf.image.resize(data_entry['segmentation_mask'], (128, 128), 'nearest')\n",
        "  image, seg_mask = normalize(image, seg_mask)\n",
        "\n",
        "  return image, seg_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjG7rCnbQHbd"
      },
      "source": [
        "  We will now map the utility functions defined above to each entry within the dataset. Note that `tf.data.experimental.AUTOTUNE` will perform calls in parallel as opposed to sequentially depending on the available CPU resources which will overall speed up the execution time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bigRuzKOpsM4"
      },
      "outputs": [],
      "source": [
        "train = dataset['train'].map(process_training, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test = dataset['test'].map(process_test, num_parallel_calls=tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6xUrFCHrWvE"
      },
      "outputs": [],
      "source": [
        "buffer_size = 1000\n",
        "batch_size = 64\n",
        "\n",
        "train_ds = train.cache().shuffle(buffer_size).batch(batch_size).repeat().prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4yWgKdMs7y-"
      },
      "source": [
        "## Define the Model\n",
        "\n",
        "Below is an image displaying the overall architecture of the UNet model we will be constructing. It consists of an desampling encoder and upsampling decoder seperated by a pipeline stage at the bottom. The gray arrows represent the link connection between corresponding encoder and decoder blocks. It may look complex at first glance but we can implement this with the help of the TensorFlow Functional API.\n",
        "<img src='https://drive.google.com/uc?export=view&id=1BeQSKL2Eq6Fw9iRXsN1hgunY-CS2nH7V' alt='unet'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo4Bn9hXF-2i"
      },
      "source": [
        "From the left side of the UNet you can see that a single block within the encoder consists of 2 Conv2D layers activated by a ReLU followed by a MaxPool and Dropout layer. Note that each subsequent block contains a higher numbers of filters for the convolutional layers. Since we have to save the outputs of the Conv2D layers before they pass through the MaxPool + Dropout for the decoder later on, we define two seperate functions. `conv_block` will simply define the ReLU activated convolutional layers and `encoder_block` will build upon its function call, chaining together the pooling and dropout layers. This way we can save the output of the `conv_block` for the decoder and build upon it for the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCAMrxftsTnd"
      },
      "outputs": [],
      "source": [
        "def conv_block(input, num_filters):\n",
        "  x = input\n",
        "\n",
        "  for i in range(2):\n",
        "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3,3), padding='same', kernel_initializer = 'he_normal')(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "  return x\n",
        "\n",
        "def encoder_block(input, num_filters):\n",
        "  a = conv_block(input, num_filters)\n",
        "  b = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(a)\n",
        "  b = tf.keras.layers.Dropout(0.3)(b)\n",
        "\n",
        "  return a, b\n",
        "\n",
        "def encoder(input):\n",
        "  a1, b1 = encoder_block(input, num_filters=64)\n",
        "  a2, b2 = encoder_block(b1, num_filters=128)\n",
        "  a3, b3 = encoder_block(b2, num_filters=256)\n",
        "  a4, b4 = encoder_block(b3, num_filters=512)\n",
        "\n",
        "  return b4, (a1, a2, a3, a4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCffizHzJfTT"
      },
      "source": [
        "The pipeline stage follows the encoder and is simply another conv block with a large number of filters designed to extract higher level features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsRKI6NlyMfW"
      },
      "outputs": [],
      "source": [
        "def pipeline(input):\n",
        "  conn_pipeline = conv_block(input, num_filters=1024);\n",
        "  return conn_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuYEOd4DJ7XY"
      },
      "source": [
        "The last piece is the decoder that upsamples the features to original size and does the pixel-wise predictions. At each step, you take the output of the previous block, upsample it (Conv2DTranspose) and concatenate with the corresponding encoder block before sending it off to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmBHBoNsy1Gz"
      },
      "outputs": [],
      "source": [
        "def decoder_block(input, conv_output, num_filters):\n",
        "  c = tf.keras.layers.Conv2DTranspose(num_filters, kernel_size=(3,3), strides=(2,2), padding='same')(input)\n",
        "  d = tf.keras.layers.concatenate([c, conv_output])\n",
        "  d = tf.keras.layers.Dropout(0.3)(d)\n",
        "  d = conv_block(d, num_filters)\n",
        "\n",
        "  return d\n",
        "\n",
        "def decoder(input, convs, output_maps):\n",
        "  b1, b2, b3, b4 = convs\n",
        "\n",
        "  d1 = decoder_block(input, b4, 512)\n",
        "  d2 = decoder_block(d1, b3, 256)\n",
        "  d3 = decoder_block(d2, b2, 128)\n",
        "  d4 = decoder_block(d3, b1, 64)\n",
        "\n",
        "  final_outputs = tf.keras.layers.Conv2D(output_maps, kernel_size=(1,1), activation='softmax')(d4)\n",
        "\n",
        "  return final_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEZPif_x2BHj"
      },
      "source": [
        "\n",
        "Now that we have defined all of our components, we can string the encoder, pipeline and decoder. Note that we use `len(class_names)` as the\n",
        "parameter value for number of output_maps since there are 3 possible labels for our use case `['pet', 'outline', 'background']`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7PNHt8L2CmU"
      },
      "outputs": [],
      "source": [
        "def unet_model():\n",
        "  inputs = tf.keras.layers.Input(shape=(128, 128, 3))\n",
        "\n",
        "  encoder_output, convs = encoder(inputs)\n",
        "\n",
        "  conn_pipeline = pipeline(encoder_output)\n",
        "\n",
        "  outputs = decoder(conn_pipeline, convs, len(class_names))\n",
        "\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "  return model\n",
        "\n",
        "model = unet_model()\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b_OtOW2WQYr"
      },
      "source": [
        "## Compile, Fit and Analyze the Model\n",
        "\n",
        "Now that everything is ready, we can begin training our model! Note that this will take approximately 15-20 minutes to complete and you can leave it running in the background\n",
        ". We will be using the adoptive Adam's optimizer and `sparse_categorical_crossentropy` as our loss since our network is assigning each pixel a multi-class prediction, [0, 1, 2] for our 3 classes/channels. Expect a val_accuracy of atleast **85%** or higher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MARk43Ej2zE9"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "training_examples = info.splits['train'].num_examples\n",
        "steps_per_epoch = training_examples // batch_size\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=test_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0ocsB24QJAP"
      },
      "source": [
        "Running the cell below will create a graph displaying the training (blue) and validation (red) metrics of the model at each epoch during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwRORqOezF0p"
      },
      "outputs": [],
      "source": [
        "display = ['loss', 'accuracy']\n",
        "\n",
        "plt.figure(figsize=(12, 3))\n",
        "for i in range(len(display)):\n",
        "  plt.subplot(1, 2, i+1)\n",
        "  plt.title(f\"Training and Validation {display[i].title()} per Epoch\")\n",
        "  plt.ylabel(display[i]); plt.xlabel('epoch')\n",
        "  plt.xticks(np.arange(0,15))\n",
        "  plt.plot(history.history[display[i]], color='b', label='train_' + display[i])\n",
        "  plt.plot(history.history['val_' + display[i]], color='r', label='val_' + display[i])\n",
        "  plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jYUjQfeD48N"
      },
      "source": [
        "# Make a Prediction\n",
        "\n",
        "Now that the model is trained, we are ready to make some predictions. The utility functions below will help process the test dataset that we defined earlier and will feed as input to the model, examples it has never seen before to determine how well it generalizes to new data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiKCtgNH6oLz"
      },
      "outputs": [],
      "source": [
        "def get_image_and_masks():\n",
        "  num_examples = info.splits['test'].num_examples\n",
        "  dataset = test_ds.unbatch().batch(num_examples)\n",
        "\n",
        "  true_images = []\n",
        "  true_masks = []\n",
        "\n",
        "  for images, masks in dataset.take(1):\n",
        "    true_images = images.numpy()\n",
        "    true_masks = masks.numpy()\n",
        "\n",
        "  return true_images[0:(num_examples - num_examples % batch_size)], true_masks[0:(num_examples - num_examples % batch_size)]\n",
        "\n",
        "def create_label_map(mask):\n",
        "  mask = tf.argmax(mask, axis=-1)\n",
        "  mask = mask[..., tf.newaxis]\n",
        "\n",
        "  return mask[0].numpy()\n",
        "\n",
        "def make_prediction(image):\n",
        "  image = image[tf.newaxis, ...]\n",
        "  pred_mask = model.predict(image)\n",
        "  pred_mask = create_label_map(pred_mask)\n",
        "\n",
        "  return pred_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dzYKMdZzPpl"
      },
      "outputs": [],
      "source": [
        "y_true_images, y_true_masks = get_image_and_masks()\n",
        "\n",
        "pred_masks = model.predict(test_ds, steps=info.splits['test'].num_examples // batch_size)\n",
        "pred_masks = np.argmax(pred_masks, axis=-1)\n",
        "pred_masks = pred_masks[..., tf.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUxg2igUDBnm"
      },
      "source": [
        "# Calculate Class Wise Metrics\n",
        "\n",
        "The cell below will define a function to compute the IOU (intersection over union) and dice score values that help determine the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhUzjMyjz6Ol"
      },
      "outputs": [],
      "source": [
        "def mask_metrics(y_true, y_pred):\n",
        "  iou_list = []\n",
        "  dice_score_list = []\n",
        "\n",
        "  smoothening_factor = 0.00001\n",
        "  for i in range(3):\n",
        "    area_of_overlap = np.sum((y_true == i) * (y_pred == i))\n",
        "    y_true_area = np.sum((y_true == i))\n",
        "    y_pred_area = np.sum((y_pred == i))\n",
        "    combined_area = y_true_area + y_pred_area\n",
        "\n",
        "    iou = (area_of_overlap + smoothening_factor) / (combined_area - area_of_overlap + smoothening_factor)\n",
        "    iou_list.append(iou)\n",
        "\n",
        "    dice_score = 2 * ((area_of_overlap + smoothening_factor) / (combined_area + smoothening_factor))\n",
        "    dice_score_list.append(dice_score)\n",
        "\n",
        "  return iou_list, dice_score_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx_zjFtRUjJ-"
      },
      "source": [
        "# Show Results\n",
        "\n",
        "Feel free to change the value of the `random_integer` variable to anything between 0 and 3467 to pick and visualize an image from the test dataset  next to its predicted and true segmentation mask. We will also display the metric scores below to see how accurate our results are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2YbhS1w8AMS",
        "outputId": "2dd3f88a-4437-4e04-bcfd-8b9ce8a4882b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n"
          ]
        }
      ],
      "source": [
        "random_integer = 45\n",
        "\n",
        "y_pred_mask = make_prediction(y_true_images[random_integer])\n",
        "iou, dice = mask_metrics(y_true_masks[random_integer], y_pred_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbSBNQ8H9wom"
      },
      "outputs": [],
      "source": [
        "titles = [\"Image\", \"Predicted Mask\", \"True Mask\"]\n",
        "img_array = [y_true_images[random_integer], y_pred_mask, y_true_masks[random_integer]]\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "metrics = [(i, iou, dice_score) for i, (iou, dice_score) in enumerate(zip(iou, dice))]\n",
        "display_string = [\"{} IOU: {} Dice Score: {}\".format(class_names[i], iou, dice_score) for i, iou, dice_score in metrics]\n",
        "display_string = \"\\n\".join(display_string)\n",
        "\n",
        "for i in range(len(class_names)):\n",
        "  plt.subplot(1, 3, i+1)\n",
        "  plt.title(titles[i])\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  if i == 1: plt.xlabel(display_string, fontsize=12)\n",
        "  plt.imshow(img_array[i])\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
